{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a16b05",
   "metadata": {},
   "source": [
    "# Enhanced LSTM Location Order Prediction Model with Additional Features\n",
    "# This notebook uses LSTM to predict the picking order with temporal and spatial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31fe170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional, Concatenate, Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939268f",
   "metadata": {},
   "source": [
    "# ==================== STEP 1: Load and Explore Data ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f2c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: Loading Data\n",
      "============================================================\n",
      "\n",
      "Dataset Shape: (4205, 18)\n",
      "\n",
      "First few rows:\n",
      "  CUST_ORDER_ID LOCATION_ID     PART_ID TYPE  QTY         TRANSACTION_DATE  \\\n",
      "0        015619    D4000000  1097062059    O  2.0  2009-04-20 00:00:00.000   \n",
      "1        015619    D4000000  1081104001    O  1.0  2009-04-20 00:00:00.000   \n",
      "2        015619    D6000000  1040021101    O  5.0  2009-04-20 00:00:00.000   \n",
      "3        015619    D7000000  1010333101    O  5.0  2009-04-20 00:00:00.000   \n",
      "4        015619    D8358900  1097082009    O  2.0  2009-04-20 00:00:00.000   \n",
      "5      013067-1    D4000000  1083341001    O  1.0  2009-04-20 00:00:00.000   \n",
      "6        015616    E7467900  1412051001    O  1.0  2009-04-20 00:00:00.000   \n",
      "7        015398    F7000000  1030076013    O  2.0  2009-04-21 00:00:00.000   \n",
      "8        015398    D8348400  1018003001    O  4.0  2009-04-21 00:00:00.000   \n",
      "9        015398    D4000000  1019093101    O  1.0  2009-04-21 00:00:00.000   \n",
      "\n",
      "               CREATE_DATE USER_ID  PICK_HOUR  DAY_OF_WEEK ZONE AISLE_BIN  \\\n",
      "0  2009-04-20 13:42:19.737   JUDER         13            2    D   4000000   \n",
      "1  2009-04-20 13:44:21.890   JUDER         13            2    D   4000000   \n",
      "2  2009-04-20 13:47:26.993   JUDER         13            2    D   6000000   \n",
      "3  2009-04-20 14:00:18.530   JUDER         14            2    D   7000000   \n",
      "4  2009-04-20 14:01:44.093   JUDER         14            2    D   8358900   \n",
      "5  2009-04-20 14:17:18.313   JUDER         14            2    D   4000000   \n",
      "6  2009-04-20 14:31:18.440   JUDER         14            2    E   7467900   \n",
      "7  2009-04-21 11:33:44.567   JUDER         11            3    F   7000000   \n",
      "8  2009-04-21 11:36:27.293   JUDER         11            3    D   8348400   \n",
      "9  2009-04-21 11:37:59.263   JUDER         11            3    D   4000000   \n",
      "\n",
      "   ORDER_SIZE  TOTAL_ORDER_QTY PREV_LOCATION NEXT_LOCATION  \\\n",
      "0        4205          16646.0           NaN      D4000000   \n",
      "1        4205          16646.0      D4000000      D6000000   \n",
      "2        4205          16646.0      D4000000      D7000000   \n",
      "3        4205          16646.0      D6000000      D8358900   \n",
      "4        4205          16646.0      D7000000      D4000000   \n",
      "5        4205          16646.0      D8358900      E7467900   \n",
      "6        4205          16646.0      D4000000      F7000000   \n",
      "7        4205          16646.0      E7467900      D8348400   \n",
      "8        4205          16646.0      F7000000      D4000000   \n",
      "9        4205          16646.0      D8348400      E4000000   \n",
      "\n",
      "   SECONDS_SINCE_LAST_PICK  LOCATION_RANK  \n",
      "0                      NaN              1  \n",
      "1                    122.0              2  \n",
      "2                    185.0              3  \n",
      "3                    772.0              4  \n",
      "4                     86.0              5  \n",
      "5                    934.0              1  \n",
      "6                    840.0              1  \n",
      "7                  75746.0              1  \n",
      "8                    163.0              2  \n",
      "9                     92.0              3  \n",
      "\n",
      "Column Names:\n",
      "['CUST_ORDER_ID', 'LOCATION_ID', 'PART_ID', 'TYPE', 'QTY', 'TRANSACTION_DATE', 'CREATE_DATE', 'USER_ID', 'PICK_HOUR', 'DAY_OF_WEEK', 'ZONE', 'AISLE_BIN', 'ORDER_SIZE', 'TOTAL_ORDER_QTY', 'PREV_LOCATION', 'NEXT_LOCATION', 'SECONDS_SINCE_LAST_PICK', 'LOCATION_RANK']\n",
      "\n",
      "Data Types:\n",
      "CUST_ORDER_ID               object\n",
      "LOCATION_ID                 object\n",
      "PART_ID                      int64\n",
      "TYPE                        object\n",
      "QTY                        float64\n",
      "TRANSACTION_DATE            object\n",
      "CREATE_DATE                 object\n",
      "USER_ID                     object\n",
      "PICK_HOUR                    int64\n",
      "DAY_OF_WEEK                  int64\n",
      "ZONE                        object\n",
      "AISLE_BIN                   object\n",
      "ORDER_SIZE                   int64\n",
      "TOTAL_ORDER_QTY            float64\n",
      "PREV_LOCATION               object\n",
      "NEXT_LOCATION               object\n",
      "SECONDS_SINCE_LAST_PICK    float64\n",
      "LOCATION_RANK                int64\n",
      "dtype: object\n",
      "\n",
      "Basic Statistics:\n",
      "            PART_ID          QTY    PICK_HOUR  DAY_OF_WEEK  ORDER_SIZE  \\\n",
      "count  4.205000e+03  4205.000000  4205.000000  4205.000000      4205.0   \n",
      "mean   1.134877e+09     3.958621    11.987158     4.066825      4205.0   \n",
      "std    2.098670e+08     6.706409     2.078348     1.342263         0.0   \n",
      "min    1.010073e+09     1.000000     7.000000     2.000000      4205.0   \n",
      "25%    1.030096e+09     2.000000    10.000000     3.000000      4205.0   \n",
      "50%    1.058595e+09     2.000000    12.000000     4.000000      4205.0   \n",
      "75%    1.095352e+09     4.000000    14.000000     5.000000      4205.0   \n",
      "max    1.990000e+09   128.000000    17.000000     6.000000      4205.0   \n",
      "\n",
      "       TOTAL_ORDER_QTY  SECONDS_SINCE_LAST_PICK  LOCATION_RANK  \n",
      "count           4205.0             4.204000e+03    4205.000000  \n",
      "mean           16646.0             5.324617e+03       5.270868  \n",
      "std                0.0             3.978946e+04       4.347218  \n",
      "min            16646.0             6.000000e+00       1.000000  \n",
      "25%            16646.0             8.600000e+01       2.000000  \n",
      "50%            16646.0             1.410000e+02       4.000000  \n",
      "75%            16646.0             3.182500e+02       7.000000  \n",
      "max            16646.0             1.125846e+06      32.000000  \n",
      "\n",
      "Missing Values:\n",
      "CUST_ORDER_ID              0\n",
      "LOCATION_ID                0\n",
      "PART_ID                    0\n",
      "TYPE                       0\n",
      "QTY                        0\n",
      "TRANSACTION_DATE           0\n",
      "CREATE_DATE                0\n",
      "USER_ID                    0\n",
      "PICK_HOUR                  0\n",
      "DAY_OF_WEEK                0\n",
      "ZONE                       0\n",
      "AISLE_BIN                  0\n",
      "ORDER_SIZE                 0\n",
      "TOTAL_ORDER_QTY            0\n",
      "PREV_LOCATION              1\n",
      "NEXT_LOCATION              1\n",
      "SECONDS_SINCE_LAST_PICK    1\n",
      "LOCATION_RANK              0\n",
      "dtype: int64\n",
      "\n",
      "Unique Values:\n",
      "Unique Customer Orders: 777\n",
      "Unique Locations: 575\n",
      "Unique Parts: 336\n",
      "Unique Zones: 7\n",
      "Location Ranks: 1 to 32\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 1: Loading Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('newTestingData.csv')\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nUnique Values:\")\n",
    "print(f\"Unique Customer Orders: {df['CUST_ORDER_ID'].nunique()}\")\n",
    "print(f\"Unique Locations: {df['LOCATION_ID'].nunique()}\")\n",
    "print(f\"Unique Parts: {df['PART_ID'].nunique()}\")\n",
    "print(f\"Unique Zones: {df['ZONE'].nunique() if 'ZONE' in df.columns else 'N/A'}\")\n",
    "print(f\"Location Ranks: {df['LOCATION_RANK'].min()} to {df['LOCATION_RANK'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16dbd7f",
   "metadata": {},
   "source": [
    "# ==================== STEP 2: Feature Engineering ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72668f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Feature Engineering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert dates to datetime\n",
    "df['CREATE_DATE'] = pd.to_datetime(df['CREATE_DATE'])\n",
    "df['TRANSACTION_DATE'] = pd.to_datetime(df['TRANSACTION_DATE'])\n",
    "\n",
    "# Extract additional features from existing data if not present\n",
    "if 'PICK_HOUR' not in df.columns:\n",
    "    df['PICK_HOUR'] = df['CREATE_DATE'].dt.hour\n",
    "    print(\"✓ Created PICK_HOUR from CREATE_DATE\")\n",
    "\n",
    "if 'DAY_OF_WEEK' not in df.columns:\n",
    "    df['DAY_OF_WEEK'] = df['CREATE_DATE'].dt.dayofweek\n",
    "    print(\"✓ Created DAY_OF_WEEK from CREATE_DATE\")\n",
    "\n",
    "if 'ZONE' not in df.columns:\n",
    "    df['ZONE'] = df['LOCATION_ID'].str[0]\n",
    "    print(\"✓ Extracted ZONE from LOCATION_ID\")\n",
    "\n",
    "if 'AISLE_BIN' not in df.columns:\n",
    "    df['AISLE_BIN'] = df['LOCATION_ID'].str[1:]\n",
    "    print(\"✓ Extracted AISLE_BIN from LOCATION_ID\")\n",
    "\n",
    "# Handle ORDER_SIZE if not present\n",
    "if 'ORDER_SIZE' not in df.columns:\n",
    "    df['ORDER_SIZE'] = df.groupby('CUST_ORDER_ID')['LOCATION_ID'].transform('count')\n",
    "    print(\"✓ Calculated ORDER_SIZE\")\n",
    "\n",
    "# Handle TOTAL_ORDER_QTY if not present\n",
    "if 'TOTAL_ORDER_QTY' not in df.columns:\n",
    "    df['TOTAL_ORDER_QTY'] = df.groupby('CUST_ORDER_ID')['QTY'].transform('sum')\n",
    "    print(\"✓ Calculated TOTAL_ORDER_QTY\")\n",
    "\n",
    "# Create sequential features if not present\n",
    "if 'PREV_LOCATION' not in df.columns:\n",
    "    df = df.sort_values(['CUST_ORDER_ID', 'CREATE_DATE'])\n",
    "    df['PREV_LOCATION'] = df.groupby('CUST_ORDER_ID')['LOCATION_ID'].shift(1)\n",
    "    print(\"✓ Created PREV_LOCATION\")\n",
    "\n",
    "if 'NEXT_LOCATION' not in df.columns:\n",
    "    df['NEXT_LOCATION'] = df.groupby('CUST_ORDER_ID')['LOCATION_ID'].shift(-1)\n",
    "    print(\"✓ Created NEXT_LOCATION\")\n",
    "\n",
    "# Calculate time between picks if not present\n",
    "if 'SECONDS_SINCE_LAST_PICK' not in df.columns:\n",
    "    df['SECONDS_SINCE_LAST_PICK'] = df.groupby('CUST_ORDER_ID')['CREATE_DATE'].diff().dt.total_seconds()\n",
    "    df['SECONDS_SINCE_LAST_PICK'] = df['SECONDS_SINCE_LAST_PICK'].fillna(0)\n",
    "    print(\"✓ Calculated SECONDS_SINCE_LAST_PICK\")\n",
    "\n",
    "# Calculate location average rank if not present\n",
    "if 'LOCATION_AVG_RANK' not in df.columns:\n",
    "    df['LOCATION_AVG_RANK'] = df.groupby('LOCATION_ID')['LOCATION_RANK'].transform('mean')\n",
    "    print(\"✓ Calculated LOCATION_AVG_RANK\")\n",
    "\n",
    "# Zone change indicator\n",
    "df['ZONE_CHANGE'] = (df['ZONE'] != df.groupby('CUST_ORDER_ID')['ZONE'].shift(1)).astype(int)\n",
    "df.loc[df.groupby('CUST_ORDER_ID').head(1).index, 'ZONE_CHANGE'] = 0\n",
    "\n",
    "# Fill NaN values\n",
    "df['PREV_LOCATION'] = df['PREV_LOCATION'].fillna('NONE')\n",
    "df['NEXT_LOCATION'] = df['NEXT_LOCATION'].fillna('NONE')\n",
    "\n",
    "print(f\"\\nTotal features after engineering: {df.shape[1]}\")\n",
    "print(\"\\nFeature list:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50bbecb",
   "metadata": {},
   "source": [
    "# ==================== STEP 3: Encode Categorical Variables ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Encoding Categorical Variables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Label Encoders\n",
    "le_location = LabelEncoder()\n",
    "le_part = LabelEncoder()\n",
    "le_zone = LabelEncoder()\n",
    "le_user = LabelEncoder()\n",
    "le_prev_loc = LabelEncoder()\n",
    "le_next_loc = LabelEncoder()\n",
    "\n",
    "df['LOCATION_ENCODED'] = le_location.fit_transform(df['LOCATION_ID'])\n",
    "df['PART_ENCODED'] = le_part.fit_transform(df['PART_ID'])\n",
    "df['ZONE_ENCODED'] = le_zone.fit_transform(df['ZONE'])\n",
    "df['USER_ENCODED'] = le_user.fit_transform(df['USER_ID'])\n",
    "df['PREV_LOCATION_ENCODED'] = le_prev_loc.fit_transform(df['PREV_LOCATION'])\n",
    "df['NEXT_LOCATION_ENCODED'] = le_next_loc.fit_transform(df['NEXT_LOCATION'])\n",
    "\n",
    "print(f\"✓ Locations encoded: {len(le_location.classes_)} unique\")\n",
    "print(f\"✓ Parts encoded: {len(le_part.classes_)} unique\")\n",
    "print(f\"✓ Zones encoded: {len(le_zone.classes_)} unique\")\n",
    "print(f\"✓ Users encoded: {len(le_user.classes_)} unique\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['QTY', 'ORDER_SIZE', 'TOTAL_ORDER_QTY', 'PICK_HOUR', \n",
    "                      'DAY_OF_WEEK', 'SECONDS_SINCE_LAST_PICK', 'LOCATION_AVG_RANK']\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "print(\"✓ Numerical features scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239a89b",
   "metadata": {},
   "source": [
    "# ==================== STEP 4: Create Sequences for LSTM ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42966098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Creating Sequences\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort by customer order and rank to maintain sequence\n",
    "df = df.sort_values(['CUST_ORDER_ID', 'LOCATION_RANK'])\n",
    "\n",
    "# Group by customer order\n",
    "order_groups = df.groupby('CUST_ORDER_ID')\n",
    "print(f\"\\nTotal orders: {len(order_groups)}\")\n",
    "\n",
    "# Analyze sequence lengths\n",
    "sequence_lengths = order_groups.size()\n",
    "print(\"\\nSequence Length Statistics:\")\n",
    "print(sequence_lengths.describe())\n",
    "\n",
    "# Create training data\n",
    "X_sequences = []\n",
    "X_features = []\n",
    "y_ranks = []\n",
    "\n",
    "for order_id, group in order_groups:\n",
    "    group = group.sort_values('LOCATION_RANK')\n",
    "    \n",
    "    # Sequence features (for LSTM)\n",
    "    location_seq = group['LOCATION_ENCODED'].values\n",
    "    part_seq = group['PART_ENCODED'].values\n",
    "    zone_seq = group['ZONE_ENCODED'].values\n",
    "    \n",
    "    # Additional features (for dense layers)\n",
    "    for i in range(len(group)):\n",
    "        row = group.iloc[i]\n",
    "        \n",
    "        # Sequence input\n",
    "        X_sequences.append(list(zip(location_seq, part_seq, zone_seq)))\n",
    "        \n",
    "        # Additional features\n",
    "        feature_vector = [\n",
    "            row['QTY'],\n",
    "            row['ORDER_SIZE'],\n",
    "            row['TOTAL_ORDER_QTY'],\n",
    "            row['PICK_HOUR'],\n",
    "            row['DAY_OF_WEEK'],\n",
    "            row['SECONDS_SINCE_LAST_PICK'],\n",
    "            row['LOCATION_AVG_RANK'],\n",
    "            row['ZONE_CHANGE'],\n",
    "            row['ZONE_ENCODED'],\n",
    "            row['USER_ENCODED'],\n",
    "            row['PREV_LOCATION_ENCODED'],\n",
    "            row['NEXT_LOCATION_ENCODED'],\n",
    "            i  # Position in sequence\n",
    "        ]\n",
    "        X_features.append(feature_vector)\n",
    "        \n",
    "        # Target\n",
    "        y_ranks.append(row['LOCATION_RANK'] - 1)  # 0-indexed\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(X_sequences)}\")\n",
    "\n",
    "# Separate sequence components\n",
    "X_location_seq = [[item[0] for item in seq] for seq in X_sequences]\n",
    "X_part_seq = [[item[1] for item in seq] for seq in X_sequences]\n",
    "X_zone_seq = [[item[2] for item in seq] for seq in X_sequences]\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(seq) for seq in X_location_seq)\n",
    "print(f\"Maximum sequence length: {max_sequence_length}\")\n",
    "\n",
    "X_location_padded = pad_sequences(X_location_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_part_padded = pad_sequences(X_part_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_zone_padded = pad_sequences(X_zone_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_features = np.array(X_features)\n",
    "y_ranks = np.array(y_ranks)\n",
    "\n",
    "# Convert to categorical\n",
    "num_ranks = len(np.unique(y_ranks))\n",
    "print(f\"Number of rank classes: {num_ranks}\")\n",
    "y_ranks_categorical = to_categorical(y_ranks, num_classes=num_ranks)\n",
    "\n",
    "print(f\"\\nX_location shape: {X_location_padded.shape}\")\n",
    "print(f\"X_part shape: {X_part_padded.shape}\")\n",
    "print(f\"X_zone shape: {X_zone_padded.shape}\")\n",
    "print(f\"X_features shape: {X_features.shape}\")\n",
    "print(f\"y_ranks shape: {y_ranks_categorical.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92628717",
   "metadata": {},
   "source": [
    "# ==================== STEP 5: Train-Test Split ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Splitting Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(X_location_padded))\n",
    "\n",
    "X_loc_train = X_location_padded[:split_idx]\n",
    "X_loc_test = X_location_padded[split_idx:]\n",
    "\n",
    "X_part_train = X_part_padded[:split_idx]\n",
    "X_part_test = X_part_padded[split_idx:]\n",
    "\n",
    "X_zone_train = X_zone_padded[:split_idx]\n",
    "X_zone_test = X_zone_padded[split_idx:]\n",
    "\n",
    "X_feat_train = X_features[:split_idx]\n",
    "X_feat_test = X_features[split_idx:]\n",
    "\n",
    "y_train = y_ranks_categorical[:split_idx]\n",
    "y_test = y_ranks_categorical[split_idx:]\n",
    "\n",
    "print(f\"Training Set: {X_loc_train.shape[0]} samples\")\n",
    "print(f\"Test Set: {X_loc_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ee1cb",
   "metadata": {},
   "source": [
    "# ==================== STEP 6: Build Enhanced LSTM Model ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f57928",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Building Enhanced LSTM Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_enhanced_lstm_model(num_locations, num_parts, num_zones, max_len, \n",
    "                                num_features, num_classes, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    Create enhanced LSTM model with multiple inputs\n",
    "    \"\"\"\n",
    "    # Input layers\n",
    "    location_input = Input(shape=(max_len,), name='location_input')\n",
    "    part_input = Input(shape=(max_len,), name='part_input')\n",
    "    zone_input = Input(shape=(max_len,), name='zone_input')\n",
    "    features_input = Input(shape=(num_features,), name='features_input')\n",
    "    \n",
    "    # Embedding layers\n",
    "    location_embed = Embedding(num_locations + 1, embedding_dim)(location_input)\n",
    "    part_embed = Embedding(num_parts + 1, embedding_dim)(part_input)\n",
    "    zone_embed = Embedding(num_zones + 1, embedding_dim // 2)(zone_input)\n",
    "    \n",
    "    # Concatenate embeddings\n",
    "    concat_embed = Concatenate()([location_embed, part_embed, zone_embed])\n",
    "    \n",
    "    # LSTM layers\n",
    "    lstm1 = Bidirectional(LSTM(128, return_sequences=True))(concat_embed)\n",
    "    dropout1 = Dropout(0.3)(lstm1)\n",
    "    \n",
    "    lstm2 = Bidirectional(LSTM(64, return_sequences=False))(dropout1)\n",
    "    dropout2 = Dropout(0.3)(lstm2)\n",
    "    \n",
    "    # Combine LSTM output with additional features\n",
    "    combined = Concatenate()([dropout2, features_input])\n",
    "    \n",
    "    # Dense layers\n",
    "    dense1 = Dense(128, activation='relu')(combined)\n",
    "    dropout3 = Dropout(0.2)(dense1)\n",
    "    \n",
    "    dense2 = Dense(64, activation='relu')(dropout3)\n",
    "    dropout4 = Dropout(0.2)(dense2)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(num_classes, activation='softmax', name='output')(dropout4)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(\n",
    "        inputs=[location_input, part_input, zone_input, features_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "lstm_model = create_enhanced_lstm_model(\n",
    "    num_locations=len(le_location.classes_),\n",
    "    num_parts=len(le_part.classes_),\n",
    "    num_zones=len(le_zone.classes_),\n",
    "    max_len=max_sequence_length,\n",
    "    num_features=X_features.shape[1],\n",
    "    num_classes=num_ranks,\n",
    "    embedding_dim=50\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12419bc2",
   "metadata": {},
   "source": [
    "# ==================== STEP 7: Train LSTM Model ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade96ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Training Enhanced LSTM Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining started...\")\n",
    "history = lstm_model.fit(\n",
    "    [X_loc_train, X_part_train, X_zone_train, X_feat_train],\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0e361",
   "metadata": {},
   "source": [
    "# ==================== STEP 8: Model Evaluation ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 8: Model Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = lstm_model.evaluate(\n",
    "    [X_loc_test, X_part_test, X_zone_test, X_feat_test],\n",
    "    y_test,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_probs = lstm_model.predict(\n",
    "    [X_loc_test, X_part_test, X_zone_test, X_feat_test],\n",
    "    verbose=0\n",
    ")\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "# Additional metrics\n",
    "mae = mean_absolute_error(y_test_labels, y_pred)\n",
    "print(f\"\\nMean Absolute Error: {mae:.4f}\")\n",
    "\n",
    "within_1 = np.mean(np.abs(y_test_labels - y_pred) <= 1)\n",
    "within_2 = np.mean(np.abs(y_test_labels - y_pred) <= 2)\n",
    "print(f\"Accuracy within ±1 rank: {within_1:.4f}\")\n",
    "print(f\"Accuracy within ±2 ranks: {within_2:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Training History - Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Model Loss Over Time')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: Training History - Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Model Accuracy Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: Confusion Matrix (show subset if too large)\n",
    "if cm.shape[0] > 20:\n",
    "    cm_display = cm[:20, :20]\n",
    "    title_suffix = \" (First 20 ranks)\"\n",
    "else:\n",
    "    cm_display = cm\n",
    "    title_suffix = \"\"\n",
    "    \n",
    "sns.heatmap(cm_display, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0], cbar_kws={'label': 'Count'})\n",
    "axes[1, 0].set_xlabel('Predicted Rank')\n",
    "axes[1, 0].set_ylabel('Actual Rank')\n",
    "axes[1, 0].set_title(f'Confusion Matrix{title_suffix}')\n",
    "\n",
    "# Plot 4: Actual vs Predicted\n",
    "axes[1, 1].scatter(y_test_labels, y_pred, alpha=0.3, s=10)\n",
    "axes[1, 1].plot([y_test_labels.min(), y_test_labels.max()], \n",
    "                [y_test_labels.min(), y_test_labels.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Actual Rank')\n",
    "axes[1, 1].set_ylabel('Predicted Rank')\n",
    "axes[1, 1].set_title('Actual vs Predicted Ranks')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Error Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = np.abs(y_test_labels - y_pred)\n",
    "print(f\"Mean error: {errors.mean():.4f}\")\n",
    "print(f\"Median error: {np.median(errors):.4f}\")\n",
    "print(f\"Max error: {errors.max():.4f}\")\n",
    "print(f\"Std error: {errors.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac32ae2d",
   "metadata": {},
   "source": [
    "# ==================== STEP 9: Prediction Function ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dab810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 9: Creating Prediction Function\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def predict_location_order_enhanced(order_data):\n",
    "    \"\"\"\n",
    "    Predict the picking order for locations with enhanced features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    order_data : DataFrame\n",
    "        DataFrame with columns: LOCATION_ID, PART_ID, QTY, and other features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with location IDs and predicted ranks\n",
    "    \"\"\"\n",
    "    # Create a copy\n",
    "    data = order_data.copy()\n",
    "    \n",
    "    # Add derived features\n",
    "    data['ZONE'] = data['LOCATION_ID'].str[0]\n",
    "    data['ORDER_SIZE'] = len(data)\n",
    "    data['TOTAL_ORDER_QTY'] = data['QTY'].sum()\n",
    "    \n",
    "    # Use current time if not provided\n",
    "    if 'PICK_HOUR' not in data.columns:\n",
    "        data['PICK_HOUR'] = pd.Timestamp.now().hour\n",
    "    if 'DAY_OF_WEEK' not in data.columns:\n",
    "        data['DAY_OF_WEEK'] = pd.Timestamp.now().dayofweek\n",
    "    \n",
    "    # Add sequence features\n",
    "    data['PREV_LOCATION'] = 'NONE'\n",
    "    data['NEXT_LOCATION'] = 'NONE'\n",
    "    data['SECONDS_SINCE_LAST_PICK'] = 0\n",
    "    data['ZONE_CHANGE'] = 0\n",
    "    \n",
    "    # Get historical average rank for each location\n",
    "    data['LOCATION_AVG_RANK'] = data['LOCATION_ID'].map(\n",
    "        df.groupby('LOCATION_ID')['LOCATION_RANK'].mean().to_dict()\n",
    "    ).fillna(df['LOCATION_RANK'].mean())\n",
    "    \n",
    "    # Encode features\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        # Encode location\n",
    "        if row['LOCATION_ID'] in le_location.classes_:\n",
    "            loc_enc = le_location.transform([row['LOCATION_ID']])[0]\n",
    "        else:\n",
    "            loc_enc = 0\n",
    "        \n",
    "        # Encode part\n",
    "        if row['PART_ID'] in le_part.classes_:\n",
    "            part_enc = le_part.transform([row['PART_ID']])[0]\n",
    "        else:\n",
    "            part_enc = 0\n",
    "        \n",
    "        # Encode zone\n",
    "        if row['ZONE'] in le_zone.classes_:\n",
    "            zone_enc = le_zone.transform([row['ZONE']])[0]\n",
    "        else:\n",
    "            zone_enc = 0\n",
    "        \n",
    "        # Prepare sequences (all locations in order)\n",
    "        loc_seq = [loc_enc] * max_sequence_length\n",
    "        part_seq = [part_enc] * max_sequence_length\n",
    "        zone_seq = [zone_enc] * max_sequence_length\n",
    "        \n",
    "        # Scale numerical features\n",
    "        numerical_vals = scaler.transform([[\n",
    "            row['QTY'],\n",
    "            row['ORDER_SIZE'],\n",
    "            row['TOTAL_ORDER_QTY'],\n",
    "            row['PICK_HOUR'],\n",
    "            row['DAY_OF_WEEK'],\n",
    "            row['SECONDS_SINCE_LAST_PICK'],\n",
    "            row['LOCATION_AVG_RANK']\n",
    "        ]])[0]\n",
    "        \n",
    "        # Create feature vector\n",
    "        feature_vec = np.concatenate([\n",
    "            numerical_vals,\n",
    "            [0, zone_enc, 0, 0, 0, idx]  # Additional encoded features\n",
    "        ])\n",
    "        \n",
    "        # Predict\n",
    "        pred_probs = lstm_model.predict(\n",
    "            [\n",
    "                np.array([loc_seq]),\n",
    "                np.array([part_seq]),\n",
    "                np.array([zone_seq]),\n",
    "                np.array([feature_vec])\n",
    "            ],\n",
    "            verbose=0\n",
    "        )[0]\n",
    "        \n",
    "        predicted_rank = np.argmax(pred_probs) + 1\n",
    "        confidence = pred_probs.max()\n",
    "        \n",
    "        predictions.append({\n",
    "            'LOCATION_ID': row['LOCATION_ID'],\n",
    "            'PART_ID': row['PART_ID'],\n",
    "            'QTY': row['QTY'],\n",
    "            'ZONE': row['ZONE'],\n",
    "            'PREDICTED_RANK': predicted_rank,\n",
    "            'CONFIDENCE': confidence\n",
    "        })\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result_df = pd.DataFrame(predictions)\n",
    "    result_df = result_df.sort_values('PREDICTED_RANK').reset_index(drop=True)\n",
    "    result_df['SUGGESTED_ORDER'] = range(1, len(result_df) + 1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"✓ Enhanced prediction function created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d6ed3",
   "metadata": {},
   "source": [
    "# ==================== STEP 10: Example Predictions ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 10: Example Predictions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get a sample order\n",
    "sample_order_id = df['CUST_ORDER_ID'].unique()[0]\n",
    "sample_order = df[df['CUST_ORDER_ID'] == sample_order_id].copy()\n",
    "\n",
    "print(f\"\\nSample Order: {sample_order_id}\")\n",
    "print(f\"Number of locations: {len(sample_order)}\")\n",
    "\n",
    "print(\"\\nActual Order:\")\n",
    "print(sample_order[['LOCATION_ID', 'PART_ID', 'QTY', 'ZONE', 'LOCATION_RANK']].sort_values('LOCATION_RANK'))\n",
    "\n",
    "# Prepare input for prediction\n",
    "prediction_input = sample_order[['LOCATION_ID', 'PART_ID', 'QTY']].copy()\n",
    "prediction_input['PICK_HOUR'] = sample_order['PICK_HOUR'].iloc[0]\n",
    "prediction_input['DAY_OF_WEEK'] = sample_order['DAY_OF_WEEK'].iloc[0]\n",
    "\n",
    "print(\"\\nPredicted Order:\")\n",
    "predicted_order = predict_location_order_enhanced(prediction_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269dd911",
   "metadata": {},
   "source": [
    "# ==================== STEP 11: Model Saving ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 11: Saving Model and Encoders\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save model\n",
    "lstm_model.save('enhanced_location_lstm_model.h5')\n",
    "print(\"✓ Model saved as 'enhanced_location_lstm_model.h5'\")\n",
    "\n",
    "# Save encoders and scaler\n",
    "import pickle\n",
    "\n",
    "encoders = {\n",
    "    'location': le_location,\n",
    "    'part': le_part,\n",
    "    'zone': le_zone,\n",
    "    'user': le_user,\n",
    "    'prev_loc': le_prev_loc,\n",
    "    'next_loc': le_next_loc,\n",
    "    'scaler': scaler,\n",
    "    'max_sequence_length': max_sequence_length\n",
    "}\n",
    "\n",
    "with open('model_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "print(\"✓ Encoders saved as 'model_encoders.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENHANCED LSTM MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Accuracy within ±1 rank: {within_1:.4f}\")\n",
    "print(f\"Accuracy within ±2 ranks: {within_2:.4f}\")\n",
    "print(\"\\nModel uses the following features:\")\n",
    "print(\"- Location ID, Part ID, Zone\")\n",
    "print(\"- Quantity, Order Size, Total Order Quantity\")\n",
    "print(\"- Pick Hour, Day of Week\")\n",
    "print(\"- Time between picks, Zone changes\")\n",
    "print(\"- Historical location patterns\")\n",
    "print(\"- Sequential context (prev/next locations)\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
